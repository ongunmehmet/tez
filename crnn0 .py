# -*- coding: utf-8 -*-
"""CRNN0.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1DsoB_EJbuup2FKQYAiqPcxgrknDWE1vr
"""

# Commented out IPython magic to ensure Python compatibility.
from google.colab import drive
drive.mount('/gdrive')
# %cd /gdrive

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import mean_squared_error
from keras.models import Sequential
from keras.utils import plot_model
from keras.callbacks import TensorBoard
import tensorflow as tf
from keras.models import Sequential
from keras.layers import Conv1D, MaxPooling1D, LSTM, Dense, Flatten, Dropout
from keras.optimizers import Adam
from keras.callbacks import EarlyStopping
from keras.preprocessing.image import ImageDataGenerator
from sklearn.model_selection import train_test_split
from keras.regularizers import l2
from datetime import datetime, timedelta
from sklearn.preprocessing import MinMaxScaler

data = pd.read_csv('/gdrive/My Drive/Colab Notebooks/Baltic Dry Index Historical Data-covid19 daily (1).csv')
data['Date'] = pd.to_datetime(data['Date'])
data = data.sort_values('Date')

data['Date'] = pd.to_datetime(data['Date'])
data = data.sort_values('Date')
target_col = 'Value'
train_size = int(len(data) * 0.60)   # 60% for training
val_size = int(train_size * 0.2)     # 20% of train for validation
train_size = train_size - val_size
test_size = len(data) - train_size - val_size

train_data = data.iloc[:train_size]
val_data = train_data.iloc[train_size - val_size:train_size]
test_data = data.iloc[train_size + val_size:]
print(len(data),len(val_data),len(train_data),len(test_data))

scaler = MinMaxScaler()
train_data_scaled = scaler.fit_transform(train_data[[target_col]])
val_data_scaled = scaler.transform(val_data[[target_col]])
test_data_scaled = scaler.transform(test_data[[target_col]])

# Convert the data into appropriate input and output format
def create_dataset(data, look_back):
    X, y = [], []
    for i in range(len(data) - look_back):
        X.append(data[i:(i+look_back), 0])
        y.append(data[i + look_back, 0])
    return np.array(X), np.array(y)

look_back = 5  # Number of previous time steps to use as input
X_train, y_train = create_dataset(train_data_scaled, look_back)
X_val, y_val = create_dataset(val_data_scaled, look_back)
X_test, y_test = create_dataset(test_data_scaled, look_back)

print(len(data),len(val_data),len(train_data),len(test_data))

model = Sequential()
model.add(Conv1D(filters=64, kernel_size=2, activation='relu', input_shape=(look_back, 1)))
model.add(MaxPooling1D(pool_size=2))
model.add(LSTM(20, activation='relu', return_sequences=True))
model.add(Flatten())
model.add(Dense(32, activation='relu', kernel_regularizer=l2(0.01)))  # L2 regularization
model.add(Dropout(0.5))  # Dropout layer for regularization
model.add(Dense(1))  # Output layer with 1 neuron for regression

model.compile(optimizer='adam', loss='mean_squared_error')

early_stopping = EarlyStopping(monitor='val_loss', patience=10, verbose=1, restore_best_weights=True)

# Train the model with updated data splits
model.fit(X_train, y_train, epochs=200, batch_size=16,
          validation_data=(X_val, y_val), callbacks=[early_stopping], verbose=1)

# Predictions
X_test_reshaped = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)
predictions = model.predict(X_test_reshaped)

# Calculate evaluation metrics
# Calculate AIC and BIC
n = len(y_test)  # Number of data points in the test set
residuals = y_test - predictions.flatten()  # Residuals

# Calculate the sum of squared residuals
ssr = np.sum(residuals**2)

# Calculate the number of parameters in the model
num_parameters = model.count_params()

# Calculate AIC and BIC
aic = n * np.log(ssr / n) + 2 * num_parameters
bic = n * np.log(ssr / n) + num_parameters * np.log(n)

print(f'AIC: {aic:.2f}')
print(f'BIC: {bic:.2f}')
test_predictions = model.predict(X_test_reshaped)
test_rmse = np.sqrt(mean_squared_error(y_test, test_predictions))
print(f"Test RMSE: {test_rmse:.2f}")

# Inverse transform the scaled test predictions
predictions_actual = scaler.inverse_transform(predictions.reshape(-1, 1))
y_train_actual = scaler.inverse_transform(y_train.reshape(-1, 1))

# Create data frames for train data, test predictions, and real test values
train_dates = train_data['Date'].values[look_back:]
test_dates = test_data['Date'].values[look_back:]
df_train_real = pd.DataFrame({'Date': train_dates, 'Real': y_train_actual[:, 0]})
df_test_pred = pd.DataFrame({'Date': test_dates, 'Predicted': predictions_actual[:, 0]})
df_test_real = pd.DataFrame({'Date': test_dates, 'Real': test_data[target_col].values[look_back:]})

# Plotting
plt.figure(figsize=(12, 6))
plt.plot(df_train_real['Date'], df_train_real['Real'], label='Eğitim Verisi')
plt.plot(df_test_pred['Date'], df_test_pred['Predicted'], label='Tahminleme Sonucu')
plt.plot(df_test_real['Date'], df_test_real['Real'], label='Gerçek Değerler')
plt.xlabel('Tarih')
plt.ylabel('Dolar')
plt.title('CRNN 2008 Krizi Tahminleme Grafiği')
plt.legend()
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

# Visualize the model architecture
plot_model(model, show_shapes=True, to_file='/gdrive/My Drive/Colab Notebooks/crnn_model.png')  # Save to a file
# -*- coding: utf-8 -*-
"""1d-CNN5.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1azKr_KlBAvnQGGCbO4TMUvpjbRXDSboZ
"""

# Commented out IPython magic to ensure Python compatibility.
from google.colab import drive
drive.mount('/gdrive')
# %cd /gdrive

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import mean_squared_error
from keras.models import Sequential
from keras.layers import Conv1D, MaxPooling1D, Flatten, Dense
from tensorflow.keras.regularizers import l2
from tensorflow.keras.callbacks import EarlyStopping
from keras.utils import plot_model
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score
from matplotlib.dates import date2num, DateFormatter

data = pd.read_csv('/gdrive/My Drive/Colab Notebooks/Baltic Dry Index Historical Data-2008 crisis daily (1).csv')
data['Date'] = pd.to_datetime(data['Date'])
data = data.sort_values('Date')
data.set_index('Date', inplace=True)

target_col = 'Value'
train_size = int(len(data) * 0.80)
val_size = int(train_size * 0.2)
train_size = train_size - val_size
test_size = len(data) - train_size - val_size

train_data = data.iloc[:train_size]
val_data = train_data.iloc[train_size - val_size:train_size]
test_data = data.iloc[train_size + val_size:]
print(len(data),len(val_data),len(train_data),len(test_data))

scaler = MinMaxScaler()
train_data_scaled = scaler.fit_transform(train_data[[target_col]])
val_data_scaled = scaler.transform(val_data[[target_col]])
test_data_scaled = scaler.transform(test_data[[target_col]])

# Define the create_dataset function
def create_dataset(data, look_back):
    X, y = [], []
    for i in range(len(data) - look_back):
        X.append(data[i:(i+look_back), 0])
        y.append(data[i + look_back, 0])
    return np.array(X), np.array(y)

look_back = 5  # Number of previous time steps to use as input

# Creating datasets
X_train, y_train = create_dataset(train_data_scaled, look_back)
X_val, y_val = create_dataset(val_data_scaled, look_back)
X_test, y_test = create_dataset(test_data_scaled, look_back)

# Reshape the data to fit the Conv1D input shape
X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)
X_val = X_val.reshape(X_val.shape[0], X_val.shape[1], 1)
X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)

model = Sequential()
model.add(Conv1D(filters=64, kernel_size=2, activation='relu', input_shape=(look_back, 1)))
model.add(MaxPooling1D(pool_size=2))
model.add(Flatten())
model.add(Dense(50, activation='relu', kernel_regularizer=l2(0.01)))
model.add(Dense(1))

model.compile(optimizer='adam', loss='mean_squared_error')

early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)

model.fit(X_train, y_train, epochs=300, batch_size=16, verbose=1, validation_data=(X_val, y_val), callbacks=[early_stopping])

# Predictions for both train and test sets
# y_train_pred = model.predict(X_train)
y_test_pred = model.predict(X_test)

# Calculate RMSE
# train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))
test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))

# Calculate AIC and BIC
n = len(y_test)  # Number of data points in the test set
residuals = y_test - y_test_pred.flatten()  # Residuals

# Calculate the sum of squared residuals
ssr = np.sum(residuals**2)

# Calculate the number of parameters in the model
num_parameters = model.count_params()

# Calculate AIC and BIC
aic = n * np.log(ssr / n) + 2 * num_parameters
bic = n * np.log(ssr / n) + num_parameters * np.log(n)

print(f'AIC: {aic:.2f}')
print(f'BIC: {bic:.2f}')
print(f"Test RMSE: {test_rmse}")



# Inverse transform the scaled predictions
# y_train_pred_actual = scaler.inverse_transform(y_train_pred)
y_test_pred_actual = scaler.inverse_transform(y_test_pred)

# Inverse transform the scaled actual values
y_train_actual = scaler.inverse_transform(y_train.reshape(-1, 1))
y_test_actual = scaler.inverse_transform(y_test.reshape(-1, 1))

# Create data frames for train predictions, test predictions, and real test values
train_dates = train_data.index[look_back:]
test_dates = test_data.index[look_back:]
# Create data frames for train data, test predictions, and real test values
df_train_real = pd.DataFrame({'Date': train_dates, 'Real': y_train_actual[:, 0]})
df_test_pred = pd.DataFrame({'Date': test_dates, 'Predicted': y_test_pred_actual[:, 0]})
df_test_real = pd.DataFrame({'Date': test_dates, 'Real': y_test_actual[:, 0]})

# Plotting
plt.figure(figsize=(12, 6))
plt.plot(df_train_real['Date'], df_train_real['Real'], label='Eğitim Verisi')
plt.plot(df_test_real['Date'], df_test_real['Real'], label='Gerçek Değerler')
plt.plot(df_test_pred['Date'], df_test_pred['Predicted'], label='Tahminleme Verisi')
plt.xlabel('Tarih')
plt.ylabel('Dolar')
plt.title('1D CNN 2008 Krizi Tahminlemesi')
plt.legend()
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

plot_model(model, show_shapes=True, to_file='/gdrive/My Drive/Colab Notebooks/1d-CNN5_model.png')  # Save to a file